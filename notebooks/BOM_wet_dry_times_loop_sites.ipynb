{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOM_wet_dry_times_loop_sites\n",
    "\n",
    "This notebook uses gridded daily BoM data to determine wet and dry seasons based off the available range of gridded BoM data (1987 - 2013). \n",
    "\n",
    "This notebok combines all of the processing steps into a single cell so that the whole process can be looped for each of the case study sites. To see what each step does in more detail, refer to \"BOM_wet_dry_times\", which includes plotting and print-to-screen outputs.\n",
    "\n",
    "** Code dependencies **\n",
    "- csv file containing the bounding boxes for the case study site/s\n",
    "\n",
    "** Accompanying code **\n",
    "- BOM_wet_dry_times.ipynb - steps through inputs and outputs for this code. See this accompanying notebook for explanation of the functionality of this code.\n",
    "\n",
    "\n",
    "Created by Claire Krause. November 2016. Datacube version 1.1.13. Python v3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datacube<index=Index<db=PostgresDb<engine=Engine(postgresql://cek156@130.56.244.227:6432/datacube)>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the librarys we need for our analysis\n",
    "%matplotlib inline\n",
    "import datacube\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import scipy.stats\n",
    "import pandas\n",
    "import csv\n",
    "import logging\n",
    "logging.getLogger('datacube.storage.storage').setLevel(logging.ERROR)\n",
    "\n",
    "dc = datacube.Datacube(app='dc-BOMrainfall')\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the list of case study sites and bounding boxes.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name  minlat  maxlat  minlon  maxlon\n",
      "0  Blackwood2  -34.10  -32.70  116.40  118.40\n",
      "1     Garford  -29.90  -29.23  133.15  134.70\n",
      "2    Mandora1  -21.10  -18.87  120.90  127.00\n",
      "3   Mandora1a  -20.03  -18.88  120.90  123.30\n",
      "4   Murchison  -27.95  -25.49  114.75  119.30\n",
      "5         Ord  -15.93  -15.05  128.30  129.29\n",
      "6      TiTree  -23.04  -21.67  133.18  134.24\n",
      "7    Daintree  -16.39  -16.09  145.21  145.42\n",
      "8       Laura  -15.85  -14.43  143.42  144.89\n",
      "9  Blackwood1  -34.40  -33.58  115.00  116.40\n"
     ]
    }
   ],
   "source": [
    "# Set up the case study bounding box (to make the file smaller and avoid memory errors)\n",
    "# Read in a csv file with all case study bounding boxes\n",
    "names = pandas.read_csv('./case_study_sites.csv', delimiter = ',')\n",
    "print(names)\n",
    "x = len(names.index)\n",
    "iterable = list(range(0,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now loop through all the sites in our file to find the 5th and 95th percentile dry and wet years using BOM gridded daily rainfall\n",
    "\n",
    "To see each of these steps in more detail, see \"BOM_wet_dry_times\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Blackwood2\n",
      "Working on Garford\n",
      "Working on Mandora1\n",
      "Working on Mandora1a\n",
      "Working on Murchison\n",
      "Working on Ord\n",
      "Working on TiTree\n",
      "Working on Daintree\n",
      "Working on Laura\n",
      "Working on Blackwood1\n"
     ]
    }
   ],
   "source": [
    "# Set up your loop so that it analyses each of the sites in the file.\n",
    "\n",
    "for num in iterable:\n",
    "    Studysite = names.ix[num]\n",
    "    print('Working on ' + Studysite.Name)\n",
    "\n",
    "    # Now set up the query with the chosen study site\n",
    "    query = {'lat': (names.maxlat[num], names.minlat[num]), \n",
    "             'lon': (names.minlon[num], names.maxlon[num]), \n",
    "             'time': ('1987-01-01','2014-01-01') }\n",
    "\n",
    "    # Grab bom_rainfall_grids from the datacube\n",
    "    Studysite_rain = dc.load(product = 'bom_rainfall_grids', **query)\n",
    "\n",
    "    # Resample to 3 monthly data, starting with December\n",
    "    monthly_avg = Studysite_rain.resample('QS-DEC', dim = 'time', how = 'mean')\n",
    "\n",
    "    # Create an average across our whole bounding box - i.e. over lat and lon\n",
    "    monthly_avg_all = monthly_avg.mean(dim = ('latitude', 'longitude'))\n",
    "    monthly_avg_all\n",
    "\n",
    "    # Find the 95th percentile\n",
    "    threshold95 = monthly_avg_all.reduce(np.percentile, dim = 'time', q = 95)\n",
    "\n",
    "    # Find the 5th percentile\n",
    "    threshold5 = monthly_avg_all.reduce(np.percentile, dim = 'time', q = 5)\n",
    "\n",
    "    # Note that the resampling we did means that each season is labelled according to its first day \n",
    "    # e.g. 1990-12-01 is Summer starting in Dec 1990.\n",
    "    wet_seasons = monthly_avg_all.where(monthly_avg_all.rainfall>=threshold95).dropna(dim = 'time')\n",
    "    dry_seasons = monthly_avg_all.where(monthly_avg_all.rainfall<=threshold5).dropna(dim = 'time')\n",
    "\n",
    "    # Change the format so that it's YYYY-MM-DD\n",
    "    dry = np.array(dry_seasons.time, dtype = 'datetime64[D]')\n",
    "    wet = np.array(wet_seasons.time, dtype = 'datetime64[D]')\n",
    "\n",
    "    # Now add these into a new csv file for later\n",
    "    site_name = names.Name[num]\n",
    "    site = [site_name,dry,wet]\n",
    "\n",
    "    # If this is the first site, make a new file, otherwise, append the existing file\n",
    "    if num == 0:\n",
    "        with open('/g/data/p25/cek156/BOM_site_rainfall.csv','w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            header = ['name','dry','wet']\n",
    "            writer.writerow(header)\n",
    "            writer.writerow(site)\n",
    "    else:\n",
    "        with open('/g/data/p25/cek156/BOM_site_rainfall.csv','a') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerow(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:agdc]",
   "language": "python",
   "name": "conda-env-agdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
