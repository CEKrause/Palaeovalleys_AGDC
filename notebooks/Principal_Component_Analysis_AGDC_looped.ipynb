{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal_component_analysis_AGDC_looped.ipynb\n",
    "\n",
    "This code combines all of the individual datasets used previously into a principal component analysis. NDVI slope, Landsat data and band indices, elevation and gamma data are all extracted and added into the PCA. The first 6 principal components are plotted and then tested for statistical difference based on the palaeovalleys shape file. The significance of each principal component helps to determine which PC best captures palaeovalley location.\n",
    "\n",
    "** Code dependencies **\n",
    "- csv file containing the bounding boxes for the case study site/s\n",
    "- palaeovalleys 2012 shape file\n",
    "- Landsat band average netcdf files produced by \"Extract_AGDC_for_study_sites_looped\"\n",
    "- NDVI slope netcdf files created by \"Palaeovalley_NDVI_linear_regression_raijin_JanJun/JulDec.py\n",
    "\n",
    "** Accompanying code**\n",
    "- Princical_component_analysis_AGDC.ipynb - shows how this code works using a single example study site\n",
    "\n",
    "Created by Claire Krause, Datacube v 1.1.17, python v3\n",
    "\n",
    "Based on PCA code from Neil Symington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import geopandas as gp\n",
    "import datacube\n",
    "datacube.set_options(reproject_threads=1)\n",
    "dc = datacube.Datacube(app='PCA')\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import scipy.stats\n",
    "import pandas\n",
    "import csv\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up some functions to use later in the code\n",
    "def warp_geometry(geom, src_crs, dst_crs):\n",
    "    \"\"\"\n",
    "    warp geometry from src_crs to dst_crs\n",
    "    \"\"\"\n",
    "    return shapely.geometry.shape(rasterio.warp.transform_geom(src_crs, dst_crs, shapely.geometry.mapping(geom)))\n",
    "\n",
    "def geometry_mask(geom, geobox, all_touched=False, invert=False):\n",
    "    \"\"\"\n",
    "    rasterize geometry into a binary mask where pixels that overlap geometry are False\n",
    "    \"\"\"\n",
    "    return rasterio.features.geometry_mask([geom],\n",
    "                                           out_shape=geobox.shape,\n",
    "                                           transform=geobox.affine,\n",
    "                                           all_touched=all_touched,\n",
    "                                           invert=invert)\n",
    "def write_to_csv(OUTPUT_path, row, idx):\n",
    "    if idx == 0:\n",
    "        with open(OUTPUT_path,'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            header = ['name', 'ttest', 'KS_test']\n",
    "            writer.writerow(header)\n",
    "            writer.writerow(row)\n",
    "    else:\n",
    "        with open(OUTPUT_path,'a') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerow(row)\n",
    "\n",
    "def write_geotiff(filename, dataset, time_index=None, profile_override=None):\n",
    "    \"\"\"\n",
    "    Write an xarray dataset to a geotiff\n",
    "    :attr bands: ordered list of dataset names\n",
    "    :attr time_index: time index to write to file\n",
    "    :attr dataset: xarray dataset containing multiple bands to write to file\n",
    "    :attr profile_override: option dict, overrides rasterio file creation options.\n",
    "    \"\"\"\n",
    "    profile_override = profile_override or {}\n",
    "\n",
    "    dtypes = {val.dtype for val in dataset.data_vars.values()}\n",
    "    assert len(dtypes) == 1  # Check for multiple dtypes\n",
    "\n",
    "    profile = DEFAULT_PROFILE.copy()\n",
    "    profile.update({\n",
    "        'width': dataset.dims['x'],\n",
    "        'height': dataset.dims['y'],\n",
    "        'affine': dataset.affine,\n",
    "        #'crs': dataset.crs.crs_str,\n",
    "        'crs': dataset.crs,\n",
    "        'count': len(dataset.data_vars),\n",
    "        'dtype': str(dtypes.pop())\n",
    "    })\n",
    "    profile.update(profile_override)\n",
    "\n",
    "    with rasterio.open(filename, 'w', **profile) as dest:\n",
    "        for bandnum, data in enumerate(dataset.data_vars.values(), start=1):\n",
    "            #dest.write(data.isel(time=time_index).data, bandnum)\n",
    "            dest.write(data, bandnum)\n",
    "            print ('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the case study areas and shape file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the case study bounding box (to make the file smaller and avoid memory errors)\n",
    "# Read in a csv file with all case study bounding boxes\n",
    "names = pandas.read_csv('/g/data/p25/cek156/case_study_sites_small.csv', delimiter = ',')\n",
    "\n",
    "# Read in the palaeovalley 2012 shape file\n",
    "shp = gp.GeoDataFrame.from_file('/g/data/p25/cek156/Palaeovalleys_2012.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and run our PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, site in enumerate(names.Name[:-6]):\n",
    "    Studysite = names.ix[num]\n",
    "    print ('Working on ' + Studysite.Name)\n",
    "    \n",
    "    ## Check for files so we don't have to run everything multiple times\n",
    "    stats_check = False #os.path.isfile('/g/data/p25/cek156/' + site + '_PCA_stats.csv')\n",
    "    if stats_check == True:\n",
    "        print('PCA already done for ' + site)\n",
    "    else:\n",
    "        ###### Landsat ############\n",
    "        # Set up our file names to be read in\n",
    "        blue_mean = '/g/data/p25/cek156/' + site + '/' + site + '_blue_time_mean.nc'\n",
    "        green_mean = '/g/data/p25/cek156/' + site + '/' + site + '_green_time_mean.nc'\n",
    "        red_mean = '/g/data/p25/cek156/' + site + '/' + site + '_red_time_mean.nc'\n",
    "        nir_mean = '/g/data/p25/cek156/' + site + '/' + site + '_nir_time_mean.nc'\n",
    "        swir1_mean = '/g/data/p25/cek156/' + site + '/' + site + '_swir1_time_mean.nc'\n",
    "        swir2_mean = '/g/data/p25/cek156/' + site + '/' + site + '_swir2_time_mean.nc'\n",
    "\n",
    "        # We need to check that the mean files have been created before we try to read them in\n",
    "        file_checkb = os.path.isfile(blue_mean)\n",
    "        file_checkg = os.path.isfile(green_mean)\n",
    "        file_checkr = os.path.isfile(red_mean)\n",
    "        file_checkn = os.path.isfile(nir_mean)\n",
    "        file_checks1 = os.path.isfile(swir1_mean)\n",
    "        file_checks2 = os.path.isfile(swir2_mean)\n",
    "\n",
    "        # If all the files are then, then we can read them in\n",
    "        if file_checkb == True & file_checkg == True & file_checkr == True & file_checkn == True & file_checks1 == True & file_checks2 == True:\n",
    "            blue = xr.open_dataset(blue_mean)\n",
    "            green = xr.open_dataset(green_mean)\n",
    "            red = xr.open_dataset(red_mean)\n",
    "            nir = xr.open_dataset(nir_mean)\n",
    "            swir1 = xr.open_dataset(swir1_mean) \n",
    "            swir2 = xr.open_dataset(swir2_mean)\n",
    "\n",
    "            ########### Set up the analyses you would like to do ###########################\n",
    "            # Check out http://www.indexdatabase.de/db/i.php for a list of a whole bunch of indices, \n",
    "            # as well as their platform specific formulas (if you click on the index name).\n",
    "            analyses = {'only_blue':blue.blue,\n",
    "            'only_green':green.green,\n",
    "            'only_red':red.red,\n",
    "            'only_nir':nir.nir,\n",
    "            'only_swir1':swir1.swir1,\n",
    "            'only_swir2':swir2.swir2,\n",
    "            'greenness':(green.green / red.red),\n",
    "            'drought':(swir2.swir2 / nir.nir),\n",
    "            'ferrous':(swir1.swir1 / nir.nir),\n",
    "            'clay':(swir1.swir1 / swir2.swir2),\n",
    "            'soilBG':(nir.nir - (2.4 * red.red)), # Soil background line\n",
    "            'soilComp':((swir1.swir1 - nir.nir) / (swir1.swir1 + nir.nir)), # Soil composition index\n",
    "            'SAVI':(((nir.nir - red.red) / (nir.nir + red.red + 0.5))*(1 + 0.5)), #Soil adjusted vegetation index, where L = 0.5\n",
    "            'FalseCol':((nir.nir + red.red + green.green)), #False colour\n",
    "            'RealCol':((red.red + green.green + blue.blue)), #Real colour\n",
    "            'NDMI':((nir.nir - swir1.swir1) / (nir.nir + swir1.swir1)), #normalised difference moisture index\n",
    "            'NDSI':((swir1.swir1 - swir2.swir2) / (swir1.swir1 + swir2.swir2))} # normalised difference salinity index\n",
    "            ################################################################################\n",
    "\n",
    "            Landsat = xr.Dataset(analyses)\n",
    "\n",
    "        ###### NDVI slope #############    \n",
    "        # Grab the Jan - June data\n",
    "        input_filename = '/g/data/p25/cek156/NDVI/' + Studysite.Name + '/' + Studysite.Name + '_NDVI_slope_JanJun.nc'\n",
    "        NDVIdataJanJun = xr.open_dataset(input_filename)\n",
    "        ## Grab the Jul - Dec data\n",
    "        input_filename = '/g/data/p25/cek156/NDVI/' + Studysite.Name + '/' + Studysite.Name + '_NDVI_slope_JulDec.nc'\n",
    "        NDVIdataJulDec = xr.open_dataset(input_filename)\n",
    "\n",
    "        ###### Elevation ##########\n",
    "        # Read in the elevation data from the datacube\n",
    "        query = {'lat': (names.maxlat[num], names.minlat[num]), \n",
    "             'lon': (names.minlon[num], names.maxlon[num]),\n",
    "             'resolution': (-250, 250), 'output_crs': 'EPSG:3577'}\n",
    "        elev = dc.load(product = 'srtm_dem1sv1_0', group_by='solar_day', **query)\n",
    "        elev = elev.dem_s\n",
    "        elev = elev.squeeze()\n",
    "\n",
    "        ###### Gamma ##########\n",
    "        # Read in the data from the datacube\n",
    "        query = {'lat': (names.maxlat[num], names.minlat[num]), \n",
    "           'lon': (names.minlon[num], names.maxlon[num]),\n",
    "            'resolution': (-250, 250), 'output_crs': 'EPSG:3577'}\n",
    "        Gamma = dc.load(product = 'gamma_ray', group_by='solar_day', **query)\n",
    "\n",
    "        ###### Merge all the data together #############\n",
    "        all_data = Landsat.copy()\n",
    "        all_data['elev'] = elev\n",
    "        all_data['NDVIJJ'] = NDVIdataJanJun.slope\n",
    "        all_data['NDVIJD'] = NDVIdataJulDec.slope\n",
    "\n",
    "        all_data = all_data.merge(Gamma)\n",
    "        all_data = all_data.squeeze()\n",
    "\n",
    "        for i, items in enumerate(all_data.data_vars.keys()):\n",
    "            #print(all_data[items])\n",
    "            a = np.array(all_data[items])\n",
    "            a = a.flatten()\n",
    "            if i == 0:\n",
    "                input_arrays = np.array(a)\n",
    "            else:\n",
    "                input_arrays = np.vstack((input_arrays, a))\n",
    "\n",
    "        ##### Remove NaNs and replace with the mode #################\n",
    "        imp = Imputer(missing_values='NaN', strategy='most_frequent', axis = 0)\n",
    "        imp.fit(input_arrays)\n",
    "        masked_data = imp.transform(input_arrays)\n",
    "\n",
    "        ####### Now run the PCA! ####################\n",
    "        #Scale each unit vector\n",
    "        scaled_data = scale(masked_data)\n",
    "        shapes = all_data.only_blue.shape\n",
    "        n_components = len(scaled_data)\n",
    "        pca = PCA(n_components = n_components)\n",
    "        pca.fit(scaled_data)\n",
    "        PC_grids = {}\n",
    "        #Reshape the principle components into the original grid shape and add them to a dictionary called PCs\n",
    "        for i in range(n_components):\n",
    "            PC_grids['PC' + str(i+1)] = pca.components_[i].reshape(shapes) \n",
    "\n",
    "        ##### Check for statistical difference and save csv file ###################################\n",
    "        # Create a bounding box from the locations specified above\n",
    "        box = shapely.geometry.box(names.minlon[num], names.minlat[num], names.maxlon[num], names.maxlat[num], ccw = True)\n",
    "        # Only get the polygons that intersect the bounding box (i.e. remove all the irrelevant ones)\n",
    "        filtered = shp.where(shp.intersects(box)).dropna()\n",
    "        # Combine all of the relevant polygons into a single polygon\n",
    "        shp_union = shapely.ops.unary_union(filtered.geometry)\n",
    "        wanted_keys = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6']\n",
    "        Only6 = dict((k, PC_grids[k]) for k in wanted_keys if k in PC_grids)\n",
    "        for idx, item in enumerate(Only6):\n",
    "            data = PC_grids[item]\n",
    "            print(item, idx)\n",
    "            OUTPUT = '/g/data/p25/cek156/' + site + '_PCA_stats.csv'\n",
    "\n",
    "            # Check for the geobox attribute. If it's not there, apply it from the datacube.\n",
    "            if not hasattr(data, 'geobox'):\n",
    "                query = {'lat': (names.maxlat[num], names.minlat[num]), \n",
    "                     'lon': (names.minlon[num], names.maxlon[num]),\n",
    "                     'resolution': (-250, 250), 'output_crs': 'EPSG:3577'}\n",
    "                elev = dc.load(product = 'srtm_dem1sv1_0', group_by='solar_day', **query)\n",
    "                print('Applying datacube geobox')\n",
    "            # Create the mask based on our shapefile\n",
    "            mask = geometry_mask(warp_geometry(shp_union, shp.crs, elev.crs.wkt), elev.geobox, invert=True)\n",
    "            # Get data only where the mask is 'true'\n",
    "            data_masked = data[np.where(mask)]\n",
    "            # Get data only where the mask is 'false'\n",
    "            data_maskedF = data[np.where(~ mask)]\n",
    "\n",
    "            ## Now check for statistical significance\n",
    "            # Create a new numpy array with just the values\n",
    "            data_masked2 = np.array(data_masked)\n",
    "            data_maskedF2 = np.array(data_maskedF)\n",
    "            # Remove nan values\n",
    "            data_masked_nonan = data_masked2[~np.isnan(data_masked2)]\n",
    "            data_maskedF_nonan = data_maskedF2[~np.isnan(data_maskedF2)]\n",
    "            masked_both = [data_masked_nonan,data_maskedF_nonan]\n",
    "            if data_masked_nonan.any():\n",
    "            # How many data points are in each of my NDVI lists?\n",
    "                size = ([len(i) for i in masked_both])\n",
    "                # Test with a t-test\n",
    "                stats_ttest, ttest_pval = scipy.stats.ttest_ind(data_masked_nonan,data_maskedF_nonan, equal_var = 'False')\n",
    "                # Test with a Kolmogorov-Smirnov test \n",
    "                # Our null hypothesis that 2 independent samples are drawn from the same continuous distribution\n",
    "                stats_KS, KS_pval = scipy.stats.ks_2samp(data_masked_nonan,data_maskedF_nonan)\n",
    "\n",
    "                # Write to csv file\n",
    "                row = [item, stats_ttest, stats_KS]\n",
    "                # Write our stats to a csv file so we can compare them later\n",
    "                # If this is the first site, make a new file, otherwise, append the existing file\n",
    "                print('writing to csv')\n",
    "                write_to_csv(OUTPUT, row, idx)\n",
    "                # Or if there is no data...\n",
    "            else:\n",
    "                print('no useful data')\n",
    "                row = [item, 'nan', 'nan']\n",
    "                write_to_csv(OUTPUT, row, idx)\n",
    "\n",
    "        ########## Plot and save ##############################\n",
    "        #Read in the CSV with the stats\n",
    "        PC_stats = pandas.read_csv(OUTPUT)\n",
    "        # Sort the data alphabetically\n",
    "        PC_stats = PC_stats.sort_values(by = 'name')\n",
    "        # Setting the positions and width for the bars\n",
    "        pos = list(range(len(PC_stats.ttest)))\n",
    "        width = 0.25\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "        ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "        PC_stats.KS_test.plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "        PC_stats.ttest.plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "        # Set the y axis label\n",
    "        ax.set_ylabel('KS test statistic', color='red')\n",
    "        ax2.set_ylabel('t test statistic', color='blue')\n",
    "        # Set the labels for the x ticks\n",
    "        ax.set_xticklabels(PC_stats['name'])\n",
    "        ax2.set_ylim([300, -300])\n",
    "        ax.set_ylim([1, -1])\n",
    "        #Let's save the plot\n",
    "        fig.savefig('/g/data/p25/cek156/' + site + '_PCA_stats.jpg', bbox_inches='tight')\n",
    "\n",
    "        ##### Save PCA plot with the greatest ttest statistic #######\n",
    "        max_PCA_val = max(PC_stats.ttest.min(), PC_stats.ttest.max(), key=abs)\n",
    "        max_PCA = PC_stats.name[PC_stats['ttest'] == max_PCA_val]\n",
    "        max_PCA = max_PCA.values[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        ax.imshow(PC_grids[str(max_PCA)])\n",
    "        ax.set_title(str(max_PCA))\n",
    "        fig.savefig('/g/data/p25/cek156/' + site + '_PCA' + max_PCA + '.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
